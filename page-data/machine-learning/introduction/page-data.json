{"componentChunkName":"component---src-templates-post-template-js","path":"/machine-learning/introduction","webpackCompilationHash":"dc0038054866dbe07008","result":{"data":{"markdownRemark":{"id":"c950c798-553f-5a86-9563-7d16948ca95b","html":"<h1 id=\"machine-learning---introduction\"><a href=\"#machine-learning---introduction\" aria-label=\"machine learning   introduction permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Machine Learning - Introduction</h1>\n<h2 id=\"tldr\"><a href=\"#tldr\" aria-label=\"tldr permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>TL;DR</h2>\n<ul>\n<li>\n<p>Teaching a machine to learn its task without being explicitly programmed</p>\n<ul>\n<li>The machine is practicing/learning on its own to improve its performance.</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"supervised-learning\"><a href=\"#supervised-learning\" aria-label=\"supervised learning permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Supervised Learning</h2>\n<ol>\n<li>\n<p>right answers are given for a data set</p>\n<ul>\n<li>so do your best to produce a right answer for another input</li>\n</ul>\n</li>\n<li>\n<p>Types of supervised learning:</p>\n<ul>\n<li>\n<p>Regression: Predict continuous valued output</p>\n<ul>\n<li>something like house prices.</li>\n</ul>\n</li>\n<li>\n<p>Classification: Discrete answers</p>\n<ul>\n<li>either 0 or 1 : true or false</li>\n<li>could be also 0, 1, 2, 3</li>\n<li>what is the probability that the input turns out to be true or false?</li>\n<li>if you need to consider more than one parameter, the graph may look different</li>\n</ul>\n</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"unsupervised-learning\"><a href=\"#unsupervised-learning\" aria-label=\"unsupervised learning permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Unsupervised Learning</h2>\n<ol>\n<li>\n<p>No right answers given</p>\n<ul>\n<li>no labeling for a data.</li>\n<li>don’t know if it’s true or false</li>\n<li>but there are some sorts of clusters: you could organize them</li>\n</ul>\n</li>\n<li>\n<p>something like organizing news.</p>\n<ul>\n<li>not given how each article is related, but Google somehow organizes them by headline</li>\n<li>you have to find the relationship between/among the given data set</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"model-representation\"><a href=\"#model-representation\" aria-label=\"model representation permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Model Representation</h2>\n<ol>\n<li>\n<p>Notation</p>\n<ul>\n<li>training set: a dataset given to train a model</li>\n<li>m : # of training examples</li>\n<li>x: input variables/features</li>\n<li>y: output variable/ target variable</li>\n<li>(x<sup>i</sup>, y<sup>i</sup>) = ith training example (ith row from the training set table)</li>\n</ul>\n</li>\n<li>\n<p>Training set -> Fed into learning algorithm -> hypothesis(h)</p>\n<ul>\n<li>hypothesis takes an input and produce an output</li>\n<li>h maps from x’s to y’s</li>\n<li>how to represent h ?</li>\n<li>h<sub>θ</sub>(x) = h(x) = θ<sub>0</sub> + θ<sub>1</sub>x (for a linear function, think of it like a f(x))</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"cost-function\"><a href=\"#cost-function\" aria-label=\"cost function permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Cost Function</h2>\n<ol>\n<li>\n<p>Measures the performance of a machine learning model: the goal is to find thetas that minimize the cost function</p>\n<ul>\n<li>θ<sub>0</sub> and θ<sub>1</sub> are parameters</li>\n<li>x<sub>1</sub> and x<sub>2</sub> are features</li>\n<li>choose the best θ<sub>0</sub> and θ<sub>1</sub> to make it close to y as much as possible</li>\n<li>so minimize (1/2m) * SIGMA ((h<sub>θ</sub>(x)-y)<sup>2</sup>)(sum of these is the cost function I think)</li>\n</ul>\n</li>\n<li>\n<p>Finding θ<sub>1</sub> which minimizes J(θ) when θ<sub>0</sub> = 0</p>\n<ul>\n<li>this is where differential equation comes in.</li>\n<li>minimum when differential = 0</li>\n</ul>\n</li>\n<li>Contour plots are used to indicate cost functions</li>\n</ol>\n<h2 id=\"gradient-descent\"><a href=\"#gradient-descent\" aria-label=\"gradient descent permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Gradient Descent</h2>\n<ol>\n<li>A way to minimize the cost function J</li>\n<li>\n<p>Start at some random θ<sub>0</sub> and θ<sub>1</sub> and keep changing them <strong>simultaneously</strong> in order to reduce J(θ<sub>0</sub>, θ<sub>1</sub>)</p>\n<ul>\n<li>θ<sub>j</sub> := θ<sub>j</sub> -α * derivative<em>with</em>respect_to*θ<sub>j</sub>(J(θ<sub>0</sub>, θ<sub>1</sub>))</li>\n<li>α is called learning rate</li>\n<li>temp0 := θ<sub>0</sub> equation</li>\n<li>temp1 := θ<sub>1</sub> equation</li>\n<li>θ<sub>0</sub> := temp0</li>\n<li>θ<sub>1</sub> := temp1</li>\n<li>same theta’s have to be used in order to calculate a temp value</li>\n</ul>\n</li>\n<li>\n<p>Intuition</p>\n<ul>\n<li>repeat until convergence: until you reach the global minimum</li>\n<li>think about the equation: θ<sub>j</sub> := θ<sub>j</sub> -α * derivative<em>with</em>respect_to*θ<sub>j</sub>(J(θ<sub>0</sub>, θ<sub>1</sub>))</li>\n<li>if the derative is positive, you know that your entire term is getting smaller, and if negative, its getting bigger</li>\n<li>if the learning rate is too small, gradient descent can be slow. because you would need more iteration</li>\n<li>if the learning rate is too large, gradient descent can overshoot the minimum -> could even diverge</li>\n<li>you don’t need to decrease the learning rate over time because the derivate term will decrease as you approach the minimum</li>\n</ul>\n</li>\n<li>\n<p>Batch</p>\n<ul>\n<li>Each step of gradient descent uses all the training examples</li>\n<li>you are computing all the sums to take the next step in gradient descent</li>\n</ul>\n</li>\n</ol>","fields":{"slug":"/machine-learning/introduction","tagSlugs":["/tag/machinelearning/","/tag/deeplearning/","/tag/supervisedlearning/","/tag/unsupervisedlearning/"]},"frontmatter":{"date":"2020-10-26T10:53:37.121Z","description":"Coursera Machine Learning course: Introduction","tags":["machinelearning","deeplearning","supervisedlearning","unsupervisedlearning"],"title":"Machine Learning - Introduction"}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/machine-learning/introduction"}}}